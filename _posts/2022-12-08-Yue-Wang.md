---
layout: post
title: "Yue Wang -- Learning 3D representations with minimal supervision"
---

{% include youtubePlayer.html yturl="https://youtu.be/p79qmoXxFNs" %}

## Bio

Yue Wang is a research scientist at Nvidia research. He graduated from MIT, working with Prof. Justin Solomon. His research interests lie in the intersection of computer vision, computer graphics, and machine learning. His major field is learning from point clouds. His paper "Dynamic Graph CNN" has been widely adopted in 3D visual computing and other fields. He is a recipient of the Nvidia Fellowship and is named the first place recipient of the William A. Martin Masterâ€™s Thesis Award for 2021. Yue received his BEng from Zhejiang University and MS from University of California, San Diego. He has spent time at Nvidia Research, Google Research and Salesforce Research.

## Abstract

Deep learning has demonstrated considerable success embedding images and more general 2D representations into compact feature spaces for downstream tasks like recognition, registration, and generation. Learning from 3D data, however, is the missing piece needed for embodied agents to perceive their surrounding environments. To bridge the gap between 3D perception and robotic intelligence, my present efforts focus on learning 3D representations with minimal supervision.   In this talk, I will discuss two key aspects to reduce the amount of human supervision in current 3D deep learning algorithms. First, I will talk about how to recover structures from 3D data such as point clouds and incorporate such inductive bias into point cloud learning pipelines. Second, I will present our works on leveraging natural supervision in point clouds to perform self-supervised learning. In addition, I will discuss how these 3D learning algorithms enable human-level perception for robotic applications such as self-driving cars.  Finally, the talk will conclude with a discussion about recent efforts to tackle neural rendering with minimal domain knowledge.  
