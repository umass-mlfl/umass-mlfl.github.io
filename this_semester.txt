{"data": [{"semester": "Fall", "year": "2025", "date": "2025-12-04", "speaker": "David Held", "website": "https://r-pad.github.io/ ", "title": "Relational Learning for Robot Manipulation", "affiliation": "CMU", "sponsor": "Oracle Labs", "video": "", "abstract": "Robots in factories today are typically confined to interact with rigid objects with known object models. How can we bring robots into the more diverse, unstructured settings of our daily lives, where objects are often deformable, articulated, or varied in shape and appearance, while maintaining high levels of performance? I argue that advancing robot capabilities requires learning methods that reason about relationships: (1) relationships between the gripper and the scene, and (2) relationships between objects being manipulated and other objects in the environment. I will show that such relational reasoning enables robots to perform complex tasks, such as deformable and articulated object manipulation, precise insertion, and non-prehensile manipulation, while generalizing to unseen objects and unseen configurations. By incorporating relational reasoning, we can achieve robust performance on challenging robot manipulation tasks.", "bio": "David Held is an Associate Professor at Carnegie Mellon University in the Robotics Institute and is the director of the RPAD lab: Robots Perceiving And Doing. His research focuses on perceptual robot learning, i.e. developing new methods at the intersection of robot perception and planning to teach robots how to manipulate novel, perceptually challenging, and deformable objects. Prior to coming to CMU, David was a post-doctoral researcher at U.C. Berkeley, and he completed his Ph.D. in Computer Science at Stanford University.  David also has a B.S. and M.S. in Mechanical Engineering at MIT.  David is a recipient of the Google Faculty Research Award in 2017 and the NSF CAREER Award in 2021. David is also leading a MURI team on the topic of \u201cCognitive and Neuroscience\u2010Inspired Problem\u2010Solving for Autonomous Systems in Physical Environments.\u201d", "area": "Robotics", "date past": "", "key": "DavidHeld", "prettyDate": "December 04"}, {"semester": "Fall", "year": "2025", "date": "2025-11-20", "speaker": "Yuanqi Du", "website": "https://yuanqidu.github.io/", "title": "", "affiliation": "", "sponsor": "Oracle Labs", "video": "", "abstract": "", "bio": "", "area": "AI4Science", "date past": "", "key": "YuanqiDu", "prettyDate": "November 20"}, {"semester": "Fall", "year": "2025", "date": "2025-11-13", "speaker": "Ryan Louie", "website": "https://youralien.github.io/ ", "title": "Adapting LLMs to Upskill Novices for High-Stakes Domains", "affiliation": "Stanford", "sponsor": "Oracle Labs", "video": "", "abstract": "Communities today suffer from demand-capacity problems in high-stakes service domains\u2014such as mental health counseling\u2014where demand for people skilled to provide essential services far exceeds supply. Can LLMs upskill novices, when expert supervision is in short supply, and ultimately scale access to human-led services in high-stakes domains?  To adapt LLM-augmented training systems to empoer novices, we must overcome several core technical challenges: (1) limited access to ML datasets due to privacy and ethical constraints; (2) need for outputs that meet the quality standards of domain-experts whose time to provide feedback is limited. This talk presents my research developing CARE, an AI system for psychotherapy skills training\u2014as a key domain in which I've addressed these technical challenges. I will highlight two methods that power efficient adaptation of LLMs for skill training in mental health: Roleplay-doh, a pipeline that converts expert critiques into Constitutional AI principles to create realistic patient simulations (EMNLP\u201924), and a co-designed feedback system that mirrors how senior therapists supervise novices, enhanced through self-improvement techniques that reduce clinically inappropriate AI suggestions (ACL\u201924). Then, I will share how we\u2019ve evaluated the educational benefits of LLM-simulated practice and feedback through conducting a randomized lab study with 90+ novice counselors. I\u2019ll end by discussing remaining questions for adapting LLM-augmented training systems to empower people to become more effective in high-stakes domains like mental health care.", "bio": "Ryan Louie is a postdoctoral researcher in the Computer Science Department at Stanford, affiliated with the Stanford NLP Group, Stanford HCI Group, and Stanford AI Lab (SAIL).  He develops NLP and HCI systems that model domain-expertise and structure effective interactions between novices and AI to realize the potential of novices to acquire skills and use them to support community well-being. Ryan completed his PhD from Northwestern University\u2019s Technology and Social Behavior program in 2023 and was awarded a Google PhD Fellowship in 2022. ", "area": "NLP, HCI", "date past": "", "key": "RyanLouie", "prettyDate": "November 13"}, {"semester": "Fall", "year": "2025", "date": "2025-11-06", "speaker": "David Burt", "website": "https://davidrburt.github.io/", "title": "Consistent Validation for Predictive Methods in Spatial Settings  ", "affiliation": "MIT", "sponsor": "Oracle Labs", "video": "", "abstract": "Spatial prediction tasks are key to weather forecasting, studying air pollution impacts, and other scientific endeavors. Determining how much to trust predictions made by statistical or physical methods is essential for the credibility of scientific conclusions. Unfortunately, classical approaches for validation fail to handle mismatch between locations available for validation and (test) locations where we want to make predictions. This mismatch is often not an instance of covariate shift (as commonly formalized) because the validation and test locations are fixed (e.g., on a grid or at select points) rather than independent and identically distributed from two distributions. We formalize a check on validation methods: that they become arbitrarily accurate as validation data becomes arbitrarily dense. We show that classical and covariate-shift methods can fail this check. We propose a method that builds from existing ideas in the covariate-shift literature, but adapts them to the validation data at hand. We prove that our proposal passes our check. And we demonstrate its advantages empirically on simulated and real data.", "bio": "David Burt is a postdoc in Professor Tamara Broderick\u2019s group at the MIT Laboratory For Information and Decision Systems. Previously,  he was a PhD student in the Machine Learning Group at the University of Cambridge, supervised by Professor Carl Edward Rasmussen. He develops methods for validation and uncertainty quantification for spatiotemporal data.", "area": "Uncertainty Quantification", "date past": "", "key": "DavidBurt", "prettyDate": "November 06"}, {"semester": "Fall", "year": "2025", "date": "2025-11-05", "speaker": "Aaron Mueller", "website": "https://aaronmueller.github.io/", "title": "Time- and Context-aware Interpretability", "affiliation": "Boston University", "sponsor": "Oracle Labs", "video": "", "abstract": "Language has a rich temporality: it is expressed and parsed incrementally through time, and meaning is iteratively composed from a sequence and its context. Interpretability research has not deeply engaged with this structure: most methods assume that mechanisms are global and static. In this talk, I will present three efforts aimed at studying the temporal and positional dynamics of language model internals. First, I will introduce methods for automatically discovering circuits\u2014task-specific subgraphs of the computation graph\u2014with explicit attention to when and where information flows across token positions. Then, I will introduce Temporal Sparse Autoencoders, which relax the standard assumption that features remain stationary over time; this method enables us to trace how concepts dynamically increase in complexity and number as context size increases. Finally, I will show a case study using garden-path sentences, where a time-aware perspective helps explain incremental sentence processing and how models revise their interpretation of a sequence as new information is received. Together, these works show that interpretability must capture not just what models represent, but also how they evolve through time. I argue that this perspective will help us build a more predictive and precise science of language model behavior and control.", "bio": "Aaron Mueller is an assistant professor of Computer Science and, by courtesy, of Data Science at Boston University. His research centers on developing language modeling methods and evaluations inspired by causal and linguistic principles, and applying these to precisely control and improve the generalization of computational models of language. He completed a Ph.D. at Johns Hopkins University and was a Zuckerman postdoctoral fellow at Northeastern and the Technion. His work has been published in ML and NLP venues (such as ICML, ACL, and EMNLP) and has won awards at TMLR and ACL. He is a recurring organizer of the BlackboxNLP and BabyLM workshops, and has recently been featured in IEEE Spectrum (2024) and MIT Technology Review (2025).", "area": "Causal Inference, LLM Interpretability", "date past": "", "key": "AaronMueller", "prettyDate": "November 05"}, {"semester": "Fall", "year": "2025", "date": "2025-10-23", "speaker": "Sherry Yang", "website": "https://sherryy.github.io/", "title": "Learning World Models and Agents for High-Cost Environments", "affiliation": "NYU Courant / Google DeepMind", "sponsor": "Oracle Labs", "video": "", "abstract": "While neural networks have achieved superhuman performance in domains with low-cost simulations\u2014from AlphaGo to LLMs\u2014their application to the physical world is bottlenecked by a fundamental challenge: high-cost interactions. In fields like robotics, ML engineering, and the natural sciences, every action or experiment is expensive and time-consuming. This talk outlines strategies for building intelligent agents that learn efficiently despite these real-world constraints. We first address the physical world by showing how learned world models can serve as high-fidelity simulators for robotics, enabling extensive policy refinement before deployment on costly hardware. We then turn to complex engineering domains, where actions like running an ML program incur significant time delays, and discuss adaptations to reinforcement learning to make it robust for these long action settings. Finally, we show how compositional generative models can navigate the vast hypothesis spaces in science, intelligently proposing experiments to accelerate the pace of discovery.", "bio": "Sherry Yang is an Assistant Professor of Computer Science at NYU Courant and a Staff Research Scientist at Google DeepMind. She researches in machine learning with a focus on reinforcement learning and generative modeling. Her current research interests include learning world models and agents, and their applications in robotics and AI for science. Her research has been recognized by the Best Paper award at ICLR and various media outlets such as VentureBeat and TWIML. She has organized tutorials, workshops, and served as Area Chairs at major conferences (NeurIPS, ICLR, ICML, CVPR). Prior to her current role, she was a post-doc at Stanford working with Percy Liang. She received her Ph.D. from UC Berkeley advised by Pieter Abbeel and Master\u2019s and Bachelor's degrees from MIT.", "area": "ML, RL, Robitics, AI4Science", "date past": "", "key": "SherryYang", "prettyDate": "October 23"}, {"semester": "Fall", "year": "2025", "date": "2025-10-16", "speaker": "Michael Boratko", "website": "https://www.mboratko.com/", "title": "Representational Capacity of Vector Embeddings for Retrieval", "affiliation": "Google DeepMind", "sponsor": "Oracle Labs", "video": "", "abstract": "Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new tasks push a fixed set of target embeddings to flexibly capture any query and notion of relevance that can be given. While prior works have pointed out various limitations of vector embeddings, there is a common assumption that these difficulties are predominantly due to unrealistically complicated queries, and those that are not can be overcome with better training data and larger models. In this talk, I will discuss why this is not the case, and demonstrate how even simple queries and documents can encounter difficulties due to representational capacity and training dynamics. These observations build on our recent work [https://arxiv.org/abs/2508.21038] which connect known results in learning theory to the setting of retrieval which imply that the number of top-k subsets of targets capable of being returned as the result of some query is limited by the dimension of the embedding. In other words, even with perfect training data and massive models, there are simply some retrieval patterns which embeddings cannot express. I will then demonstrate empirical evidence of this limitation in action. Even when training free embeddings (not subject to model constraints) we find that existing training losses struggle to model the case where any pair of documents (i.e. k=2) should be capable of being retrieved on even a moderate (eg. 300) number of targets. I will then discuss our synthetic dataset LIMIT that stress tests models based on these observations, and we find that existing even state-of-the-art models fail on this dataset despite the simple nature of the task. I will argue that these results expose a bottleneck in representational choice and training dynamics for embedding models, and discuss potential methodological improvements to address these issues going forward.", "bio": "Michael Boratko is a research scientist at Google DeepMind, where his work explores large-scale representation learning and new paradigms for retrieval, including generative retrieval and extreme long-context models. Prior to joining GDM he was a postdoctoral researcher at UMass Amherst in IESL under the direction of Andrew McCallum, where his research focused on geometric an region-based embeddings (notably box embeddings) blending set-theoretic, probabilistic, and geometric structure in learned representations. He earned his PhD in Mathematics from UMass Amherst in 2018 with work on harmonic analysis and variational techniques for geometric PDEs under advisors Andrea Nahmod and N\u00e9stor Guill\u00e9n.", "area": "Representation Learning, Retrieval, Probabilistic Modeling, NLP", "date past": "", "key": "MichaelBoratko", "prettyDate": "October 16"}, {"semester": "Fall", "year": "2025", "date": "2025-10-09", "speaker": "Wei-Chiu Ma", "website": "https://www.cs.cornell.edu/~weichiu/", "title": "Towards Physically-grounded Digital Twins and Beyond", "affiliation": "Cornell", "sponsor": "Oracle Labs", "video": "", "abstract": "Generative AI and foundation models have revolutionized numerous fields (e.g., vision, NLP), transforming our lives in many ways. However, their impact on robotics remains relatively limited compared to other domains. One critical hurdle preventing robotics from reaching the \"GPT moment\" is the lack of sufficient data. Unlike the abundant image and text data available on the web, real-world robotic data is much more scarce. Collecting this data is expensive, time-consuming, and, most importantly, presents significant safety concerns. In this context, the automatic creation of realistic, interactable, and highly detailed virtual replicas of physical environments offers immense potential. By making digital twins look real and act real, we can use them as dynamic, virtual testbeds for training and evaluating robotic agents at scale. In this talk, I will share our recent progress in advancing digital twin construction and how it enables more robust policy learning. By building replicas that are not only visually and geometrically accurate but also physically grounded, robotic agents deployed in these mirror worlds can interact with their environments and leverage observations and feedback to learn decision-making policies that transfer seamlessly to their real-world counterparts -- safely and at scale.", "bio": "Wei-Chiu Ma is an Assistant Professor of Computer Science at Cornell University. His research lies at the intersection of 3D/4D computer vision and robotics, with a focus on building AI systems that can understand, reconstruct, and re-simulate the dynamic world. Wei-Chiu is a recipient of the Siebel Scholarship and was selected as a rising star in Cyber Physical Systems. His work has been covered by media outlets such as WIRED, DeepLearning.AI, MIT News, etc. Previously, Wei-Chiu was a Sr. Research Scientist at UberATG and Waabi, where he served as the technical lead of the sensor simulation team. His contribution to autonomy and simulation have led to 15+ patents. He received his Ph.D. in EECS from MIT and his M.S. in Robotics from CMU.", "area": "Vision, Robotics", "date past": "", "key": "Wei-ChiuMa", "prettyDate": "October 09"}]}