{"data": [{"semester": "Fall", "year": "2024", "date": "2024-11-14", "speaker": "Andrew Wu", "website": "https://wu-haoze.github.io/", "title": "Marabou 2.0: A Versatile Formal Analyzer of Neural Networks", "affiliation": "Amherst College", "sponsor": "", "video": "", "abstract": "Deep neural networks are revolutionizing the way complex systems are designed. Consequently, there is a pressing need for tools and techniques to obtain formal guarantees on the behaviors of neural networks. To address that need, we present version 2.0 of Marabou, a toolkit for formally verifying user-defined properties on deep neural networks. We discuss the tool\u2019s architectural design and highlight a few of its many recent applications, in robotics, vision, and systems domains.", "bio": "Andrew (Haoze) Wu is an Assistant Professor in Computer Science at Amherst College. He recently obtained his PhD in Computer Science from Stanford University. Andrew's work aims to bring together automated reasoning and machine learning to create safer and smarter computer systems. He is the lead developer of the Marabou framework, a state-of-the-art neural network verification tool widely used in academia and industry. His work has been published at top venues in formal methods and artificial intelligence.", "area": "Automated Logical Reasoning, Formal Verification", "date past": "", "key": "AndrewWu", "prettyDate": "November 14"}, {"semester": "Fall", "year": "2024", "date": "2024-11-07", "speaker": "Claudia Shi", "website": "https://www.claudiashi.com/", "title": "Novel Problems, Classic Solutions: Understanding LLMs Through the Lens of Statistics", "affiliation": "Columbia", "sponsor": "", "video": "", "abstract": "In this talk, I will present two recent projects that use statistical methods to deepen our understanding of LLMs. First, I investigate the moral beliefs encoded within LLMs, focusing on the advice they offer in morally ambiguous scenarios. To quantify this, I developed statistical measures and metrics that define what it means for a model to make a choice and evaluate the uncertainty associated with that choice. By conducting a large-scale survey with 26 LLMs, we analyzed their responses to various morally ambiguous situations. Our findings reveal that frontier LLMs often provide similar responses, even when they differ from those of human annotators. Second, I explore how LLMs implement tasks by examining the circuit hypothesis\u2014the idea that specific tasks are executed by subnetworks within the model, known as circuits. I developed a suite of hypothesis tests based on criteria such as mechanism preservation, localization, and minimality to evaluate these circuits. Applying these tests to several circuits identified in existing research, I assess the extent to which these circuits align with the idealized concept proposed by the hypothesis.", "bio": "Claudia is a final-year Ph.D. student in Computer Science at Columbia University, advised by David Blei. Her research advances the scientific understanding of LLMs and their responsible deployment. Her work has been recognized as a spotlight paper at the Conference on Neural Information Processing Systems (NeurIPS) 2023. She is a recipient of the Columbia Center of AI Technology Ph.D. Fellowship in 2024.", "area": "Causal Inference, LLM Interpretability", "date past": "", "key": "ClaudiaShi", "prettyDate": "November 07"}, {"semester": "Fall", "year": "2024", "date": "2024-10-31", "speaker": "Boqing Gong", "website": "http://boqinggong.info/", "title": "From Domain Adaptation to VideoPrism: A Decade-Long Quest for Out-of-Domain Visual Generalization", "affiliation": "Boston University", "sponsor": "", "video": "", "abstract": "This talk explores the challenges of out-of-domain (OOD) generalization in computer vision, encompassing tasks like domain adaptation, webly-supervised learning, and long-tailed recognition. I will review some principles and techniques underlying the seemingly diverse tasks and then connect them to the recent development of generalist vision systems, showcasing VideoPrism --- a state-of-the-art generalist video encoding model --- and ongoing research into image and video generation models.", "bio": "Boqing Gong is a computer science faculty member at Boston University and a part-time research scientist at Google DeepMind. His research focuses on AI models' generalization and efficiency and the visual analytics of objects, scenes, human activities, and their interactions", "area": "Vision", "date past": "", "key": "BoqingGong", "prettyDate": "October 31"}, {"semester": "Fall", "year": "2024", "date": "2024-10-24", "speaker": "Alex Wong", "website": "https://vision.cs.yale.edu/members/alex-wong.html", "title": "The Know-How of Multimodal Depth Perception", "affiliation": "Yale", "sponsor": "", "video": "https://www.youtube.com/watch?v=sYjTx26VUVQ", "abstract": "Training deep neural networks requires tens of thousands to millions of examples, so curating multimodal vision datasets amounts to numerous man-hours; tasks like depth estimation require an even more massive effort. I will introduce an alternative form of supervision that leverages multi-sensor validation as an unsupervised (or self-supervised) training objective for depth estimation. To address its ill-posedness, I will show how one can leverage multimodal inputs in the choice of regularizers, which can play a role in model complexity, speed, generalization, as well as adaptation to test-time (possibly adverse) environments. Additionally, I will discuss the current limitations of data augmentation procedures used during unsupervised training, which involves reconstructing the inputs as the supervision signal, and detail a method that allows one to scale up and introduce previously inviable augmentations to boost performance. Finally, I will show how one can scalably expand the number of modalities supported by multimodal models and demonstrate their use in a number of downstream semantic tasks.", "bio": "Alex Wong is an Assistant Professor in the department of Computer Science and the director of the Vision Laboratory at Yale University. He also serves as the Director of AI (consulting capacity) for Horizon Surgical Systems. Prior to joining Yale, he was an Adjunct Professor at Loyola Marymount University (LMU) from 2018 to 2020. He received his Ph.D. in Computer Science from the University of California, Los Angeles (UCLA) in 2019 and was co-advised by Stefano Soatto and Alan Yuille. He was previously a post-doctoral research scholar at UCLA under the guidance of Soatto. His research lies in the intersection of machine learning, computer vision, and robotics and largely focuses on multimodal 3D reconstruction, robust vision under adverse conditions, and unsupervised learning. His work has received the outstanding student paper award at the Conference on Neural Information Processing Systems (NeurIPS) 2011 and the best paper award in robot vision at the International Conference on Robotics and Automation (ICRA) 2019.", "area": "Vision", "date past": "", "key": "AlexWong", "prettyDate": "October 24"}, {"semester": "Fall", "year": "2024", "date": "2024-10-17", "speaker": "Agustinus Kristiadi", "website": "https://agustinus.kristia.de/", "title": "Probabilistic Inference and Decision-Making With and For Foundation Models", "affiliation": "Vector Institute", "sponsor": "", "video": "https://www.youtube.com/watch?v=ZxUBIVvSP6U", "abstract": "apturing our belief about an unknown given observations. Central in this paradigm are probabilistic models and approximate inference methods. The former models one\u2019s prior belief and encodes the data, while the latter produces posterior distributions based on the former. In the era of large-scale neural networks and foundation models, leveraging them in probabilistic modeling or improving them using probabilistic inference is challenging due to their sheer size. In this talk, I will discuss recent works in (i) developing efficient probabilistic models with and for large foundation models, (ii) leveraging the resulting powerful, calibrated beliefs to improve decision-making and planning, and (iii) applying the resulting probabilistic decision-making/planning systems for improving scientific discovery, and improving the neural networks themselves.", "bio": "Probabilistic inference is a compelling framework for capturing our belief about an unknown given observations. Central in this paradigm are probabilistic models and approximate inference methods. The former models one\u2019s prior belief and encodes the data, while the latter produces posterior distributions based on the former. In the era of large-scale neural networks and foundation models, leveraging them in probabilistic modeling or improving them using probabilistic inference is challenging due to their sheer size. In this talk, I will discuss recent works in (i) developing efficient probabilistic models with and for large foundation models, (ii) leveraging the resulting powerful, calibrated beliefs to improve decision-making and planning, and (iii) applying the resulting probabilistic decision-making/planning systems for improving scientific discovery, and improving the neural networks themselves.", "area": "Probabilistic ML", "date past": "", "key": "AgustinusKristiadi", "prettyDate": "October 17"}, {"semester": "Fall", "year": "2024", "date": "2024-10-10", "speaker": "Xiaolong Wang", "website": "https://xiaolonw.github.io/", "title": "Learning Humanoid Robots", "affiliation": "UCSD", "sponsor": "", "video": "https://www.youtube.com/watch?v=lBgdDY1VBkY", "abstract": "Having a humanoid robot operating like a human has been a long-standing goal in robotics. The humanoid robot provides a generalized purpose platform to conduct diverse tasks we do in our daily lives. In this talk, we study learning-based approaches for both the mobility and manipulation skills of the humanoid robot, with the goal of generalization to diverse tasks, objects, and scenes. I will discuss how to perform whole-body control in humanoids with rich, diverse, and expressive motions. I will also share some lessons we learned from developing teleoperation systems to operate humanoid robots and collect training data. With the collected data, we aim to build the robot foundation model using a novel RNN architecture with Test-Time Training (TTT).", "bio": "Xiaolong Wang is an Assistant Professor in the ECE department at the University of California, San Diego, and a Visiting Professor at NVIDIA Research. He received his Ph.D. in Robotics at Carnegie Mellon University. His postdoctoral training was at the University of California, Berkeley. His research focuses on the intersection between computer vision and robotics. His specific interest lies in learning visual representations from videos and physical robotic interaction data. These comprehensive representations are utilized to facilitate the learning of human-like robot skills, with the goal of generalizing the robot to interact effectively with a wide range of objects and environments in the real physical world. He is the recipient of the J. K. Aggarwal Prize, NSF CAREER Award, Intel Rising Star Faculty Award, and Research Awards from Sony, Amazon, Adobe, and CISCO.", "area": "Robotics, Vision", "date past": "", "key": "XiaolongWang", "prettyDate": "October 10"}, {"semester": "Fall", "year": "2024", "date": "2024-10-03", "speaker": "Silvia Sell\u00e1n", "website": "https://www.silviasellan.com/", "title": "Stochastic Computer Graphics", "affiliation": "MIT", "sponsor": "", "video": "https://www.youtube.com/watch?v=ZhC11_xA7BQ", "abstract": "Computer Graphics research has long been dominated by the interests of large film, television and social media companies, forcing other, more safety-critical applications (e.g., medicine, engineering, security) to repurpose Graphics algorithms originally designed for entertainment. In this talk, I will advocate for a perspective shift in our field that allows us to design algorithms directly for these safety-critical application realms. I will show that this begins by reinterpreting traditional Graphics tasks (e.g., 3D modeling and reconstruction) from a statistical lens and quantifying the uncertainty in our algorithmic outputs, as exemplified by the research I have conducted for the past five years. I will end by mentioning several ongoing and future research directions that carry this statistical lens to entirely new problems in Graphics and Vision and into specific applications.", "bio": "Silvia is a current postdoc at MIT and an incoming faculty member at Columbia University, working on Computer Graphics and Geometry Processing. She is a Vanier Doctoral Scholar, an Adobe Research Fellow and the winner of the 2021 University of Toronto Arts & Science Dean\u2019s Doctoral Excellence Scholarship. She has interned twice at Adobe Research and twice at the Fields Institute of Mathematics. She is also a founder and organizer of the Toronto Geometry Colloquium and a member of WiGRAPH.", "area": "Computer Graphics", "date past": "", "key": "SilviaSell\u00e1n", "prettyDate": "October 03"}, {"semester": "Fall", "year": "2024", "date": "2024-09-26", "speaker": "Xinya Du", "website": "https://xinyadu.github.io/", "title": "Synergizing Knowledge and  Large Language Models", "affiliation": "UT Dallas", "sponsor": "", "video": "", "abstract": "Large Language Models (LLMs) have revolutionized the field of natural language processing and reshaped how humans acquire and interact with knowledge. In this talk, I will discuss my research on synergizing LMs and knowledge \u2014 where LLMs not only extract and discover knowledge, but also continually improve by integrating new knowledge. First, I will cover our work on improving knowledge extraction from the vast amount of existing literature, with a particular focus on enabling models to better understand long documents in a cost-efficient and comprehensive manner. I will describe a novel paradigm for representing document-level structured information as question answer pairs, and how we extract them by leveraging global context. Next, I will introduce our pioneering investigation into using LLMs for new scientific knowledge discovery. We explore a multi-stage, LLM-based framework to generate and iteratively refine natural language scientific hypotheses. Finally, building on the above efforts, I will complete the virtuous cycle by demonstrating how LLMs can integrate the knowledge they acquire to continuously enhance their reasoning capabilities and their ability to learn new knowledge.", "bio": "Xinya Du is a tenure-track assistant professor at UT Dallas Computer Science Department. He earned a Ph.D. degree from Cornell University and was a Postdoctoral Research Associate at the University of Illinois (UIUC). He has also worked at Microsoft Research, Google Research, and Allen Institute AI. His research is on natural language processing, deep learning, and large language models. The goal is to build intelligent machines with both faithful knowledge & reasoning capabilities.  His work has been published in leading NLP and ML conferences (ACL, EMNLP, ICLR). His work was included in the list of Most Influential ACL Papers by Paper Digest and has been covered by major media like New Scientist. He was named a Spotlight Rising Star in Data Science by the University of Chicago and was selected for the New Faculty Highlights program by AAAI. He is the recipient of the 2024 Amazon Research Award and the 2024 NSF CAREER Award.", "area": "NLP", "date past": "", "key": "XinyaDu", "prettyDate": "September 26"}, {"semester": "Fall", "year": "2024", "date": "2024-09-12", "speaker": "Yixin Wang", "website": "https://yixinwang.github.io/", "title": "Causal Inference with Unstructured Data", "affiliation": "University of Michigan", "sponsor": "", "video": "", "abstract": "Causal inference traditionally involves analyzing tabular data where variables like treatment, outcome, covariates, and colliders are manually labeled by humans. However, many complex causal inference problems rely on unstructured data sources such as images, text and videos that depict overall situations. These causal problems require a crucial first step - extracting the high-level latent causal factors from the low-level unstructured data inputs, a task known as \"causal representation learning\". In this talk, we explore how to identify latent causal factors from unstructured data, whether from passive observations, interventional experiments, or multi-domain datasets. While latent factors are classically uncovered by leveraging their statistical independence, causal representation learning grapples with a thornier challenge: the latent causal factors are often correlated, causally connected, or arbitrarily dependent.", "bio": "Yixin Wang is an assistant professor of statistics at the University of Michigan. She works in the fields of Bayesian statistics, machine learning, and causal inference. Previously, she was a postdoctoral researcher with Professor Michael Jordan at the University of California, Berkeley. She completed her PhD in statistics at Columbia, advised by Professor David Blei, and her undergraduate studies in mathematics and computer science at the Hong Kong University of Science and Technology. Her research has been recognized by the j-ISBA Blackwell-Rosenbluth Award, ICSA Conference Young Researcher Award, ISBA Savage Award Honorable Mention, ACIC Tom Ten Have Award Honorable Mention, and INFORMS data mining and COPA best paper awards.", "area": "Causal Inference", "date past": "", "key": "YixinWang", "prettyDate": "September 12"}]}