{"data": [{"semester": "Spring", "year": "2024", "date": "2024-04-25", "speaker": "Huaizu Jiang", "website": "https://jianghz.me/", "title": "", "affiliation": "Northeastern University", "sponsor": "", "video": "", "abstract": "", "bio": "", "area": "Visual intelligence, computational photography, NLP, ML", "date past": "", "key": "HuaizuJiang", "prettyDate": "April 25"}, {"semester": "Spring", "year": "2024", "date": "2024-04-18", "speaker": "Dallas Card", "website": "https://dallascard.github.io/", "title": "", "affiliation": "University of Michigan", "sponsor": "", "video": "", "abstract": "", "bio": "", "area": "ML/NLP to learn about society from text", "date past": "", "key": "DallasCard", "prettyDate": "April 18"}, {"semester": "Spring", "year": "2024", "date": "2024-04-11", "speaker": "Yining Hong", "website": "https://evelinehong.github.io/", "title": "", "affiliation": "UCLA", "sponsor": "", "video": "", "abstract": "", "bio": "", "area": "Embodied Agents, 3D World Model, Vision", "date past": "", "key": "YiningHong", "prettyDate": "April 11"}, {"semester": "Spring", "year": "2024", "date": "2024-03-27", "speaker": "Aviral Kumar", "website": "https://aviralkumar2907.github.io/", "title": "", "affiliation": "Google", "sponsor": "", "video": "", "abstract": "", "bio": "", "area": "RL", "date past": "", "key": "AviralKumar", "prettyDate": "March 27"}, {"semester": "Spring", "year": "2024", "date": "2024-03-15", "speaker": "Cheng Phoo", "website": "https://www.cs.cornell.edu/~cpphoo/", "title": "", "affiliation": "Cornell", "sponsor": "", "video": "", "abstract": "", "bio": "", "area": "Computer vision, multimodal sensory", "date past": "", "key": "ChengPhoo", "prettyDate": "March 15"}, {"semester": "Spring", "year": "2024", "date": "2024-03-07", "speaker": "Tom Hartvigsen", "website": "https://www.tomhartvigsen.com/", "title": "Lifelong Editing for Language Models", "affiliation": "UVA, MIT", "sponsor": "", "video": "https://youtu.be/HAcQcyx1yGA", "abstract": "Despite impressive capabilities, language models still generate factually incorrect, biased, and hallucinatory content. When we find such misbehaviors, we need ways to update our models while avoiding excessive finetuning costs and retaining the model behaviors we like. One burgeoning approach to keeping language models factual is model editing, which aims to inject new facts into language model weights. However, most existing methods 1) edit a model only once, despite a quickly-changing world, and 2) update language model weights directly, which can cause dramatic side-effects to model behavior. In this talk, I will introduce GRACE, our editing method that leaves a language model's weights untouched. GRACE works by learning and caching replacement activations that induce corrected language models outputs. New activations are selectively retrieved during inference when new inputs resemble prior edits. Beyond achieving edits without weight updates, GRACE unlocks success in a hard, new lifelong model editing setup, where we repeatedly edit the same model thousands of times in a row. After introducing GRACE, I will briefly showcase our other recent efforts to improve model editing evaluation and to interface model editors with other test-time interventions.", "bio": "Tom Hartvigsen is an Assistant Professor of Data Science at the University of Virginia. He works to make machine learning trustworthy, robust, and socially-responsible enough for deployment in high-stakes, dynamic settings, especially those found in healthcare. Tom\u2019s work has been published at the top venues for Machine Learning, NLP, and Data Mining including NeurIPS, ACL, KDD, and AAAI. Before joining UVA, Tom was a postdoc at MIT CSAIL. He holds a Ph.D. in Data Science from WPI and a Bachelor\u2019s in Applied Math from SUNY Geneseo.", "area": "NLP, Healthcare", "date past": "", "key": "TomHartvigsen", "prettyDate": "March 07"}, {"semester": "Spring", "year": "2024", "date": "2024-02-21", "speaker": "Michael Niemeyer", "website": "https://m-niemeyer.github.io/", "title": "Neural Representations for 3D Asset Reconstruction, Generation, and Beyond", "affiliation": "Google", "sponsor": "", "video": "https://youtu.be/M23H5cG8yJc", "abstract": "Neural field-based representations such as Neural Radiance Fields have revolutionized 3D computer vision in recent years. While they achieve impressive results in tasks such as view synthesis or generative modeling, neural fields are still only reluctantly used and integrated into larger-scale graphics, animation, or simulation software and products. Common reasons are slow and compute-intense inference and the reliance on ray-marching instead of following the more traditional rasterization pipeline that many modern compute devices are built for. In this talk, we first investigate the reconstruction use case: How can radiance field representations be transformed into mesh-based representations? And how can this be efficiently done in challenging scenarios such as highly-reflective surfaces? Next, we shift focus to the generative use case: We investigate how 3D assets can be generated purely from text prompts as well as how subject-driven reconstructions can be obtained from collections of images. Finally, we discuss first attempts of how neural fields can be used for SLAM to achieve real-time reconstructions from input image streams.", "bio": "Michael is a research scientist at Google working on 3D computer vision and generative modeling in Federico Tombari\u2019s lab. In 2022, he completed his PhD at the Max Planck Institute in Tuebingen supervised by Andreas Geiger. The works Occupancy Networks and Differentiable Volumetric Rendering were selected among the top 15 most influential papers at CVPR 2019 and 2020, and he received the best paper award at CVPR 2021 for the GIRAFFE project.", "area": "3D Vision/Computer Vision", "date past": "", "key": "MichaelNiemeyer", "prettyDate": "February 21"}, {"semester": "Spring", "year": "2024", "date": "2024-02-15", "speaker": "Tan Zhi-Xuan", "website": "https://ztangent.github.io/", "title": "Real-Time Open-Ended Goal Inference from Actions and Language via Bayesian Inverse Planning", "affiliation": "MIT", "sponsor": "", "video": "", "abstract": "People routinely infer the goals and intentions of others from both actions and words. How might we build assistive machines that do the same? This talk will first introduce Bayesian inverse planning as a general framework for goal inference. I will then show how these problems can be solved accurately and efficiently via sequential inverse plan/policy search (SIPS), a family of algorithms that model agents as online model-based planners, and use programmable particle filtering to rapidly infer agents' goals and plans from observations of their behavior. Through the use of both incremental algorithms and compiler optimizations for model-based planning, SIPS can be made to run in (faster than) real-time. Because SIPS is implemented using probabilistic programming, it is highly configurable. For example, SIPS can be used to model boundedly-rational agents, allowing us to infer an agent's goals even when they make planning mistakes. SIPS can also handle language input: By using large language models (LLMs) as likelihood functions over how people communicate their plans in natural language, SIPS can infer human plans from incomplete or ambiguous instructions. Finally, SIPS can be integrated with conditional priors over human goals that are learned from data, allowing us to scale online goal inference to open-ended settings with hundreds of possible goals. These advances pave the way towards fast, flexible, and grounded inferences over the infinite variety of human goals, furthering the development of human-aligned assistive systems.", "bio": "Tan Zhi-Xuan is a 5th year PhD student in the Computational Cognitive Science and Probabilistic Computing Groups at MIT. Their research sits at the intersection of AI and cognitive science, asking questions like: How can we specify and perform inference over rich yet structured generative models of human decision-making, in order to accurately infer human goals, values, and norms? To answer these questions, Xuan's work includes the development of probabilistic programming and model-based planning infrastructure, so as to enable fast and flexible Bayesian inference over complex models of agents and their environments.", "area": "Probabilistic programming, computational cognitive science", "date past": "", "key": "TanZhi-Xuan", "prettyDate": "February 15"}, {"semester": "Spring", "year": "2024", "date": "2024-02-09", "speaker": "Hao Dong", "website": "https://zsdonghao.github.io/", "title": "Towards Unified Robotics Manipulation via Object-centric Policy", "affiliation": "Peking University", "sponsor": "", "video": "", "abstract": "Enabling robots to manipulate everyday objects is a key focus in embodied intelligence research. This area presents challenges due to the diversity of tasks and the varying shapes and structures of objects. An ideal robotic manipulation policy should provide a unified representation that is adaptable to different objects and tasks. In this presentation, we introduce the concept of object-centric manipulation policy through affordance learning, which offers a unified way to represent policies that are suitable for a wide range of objects and tasks. By leveraging affordance, we decouple the relationship between robots and objects, enabling different robots to perform the same tasks using the same action trajectories.", "bio": "Hao Dong is an Assistant Professor at Peking University studying AI, robotics and vision. His current research focuses on several exciting areas: generalizable robotic manipulation, dexterous manipulation, robot vision, and autonomous decision making. The ultimate goal is to create cost-effective and autonomous robots for both industrial applications and home assistance scenarios. ", "area": "Robotics, Vision", "date past": "Past", "key": "HaoDong", "prettyDate": "February 09"}]}